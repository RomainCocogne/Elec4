{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PNS ELEC4 EIEL821 Spectral Analysis \n",
    "\n",
    "# Chapter 1: Random processes - solutions\n",
    "\n",
    "This notebook reviews some basic concepts of stochastic processes, including stationarity and ergodicity.\n",
    "\n",
    "## Random realizations\n",
    "\n",
    "* Generate 100 realizations of the random process \n",
    "\n",
    "$$X(t) = sin(w_0t+\\theta)$$\n",
    "\n",
    "with $w_0 = 2\\pi/100$, where each realization is composed of 200 samples, $t = 0, 1, \\dots, 199$. Random variable $\\theta$ is uniformly distributed in the interval $[0, 2\\pi[$. The samples of the $i$th realization will be stored along the $i$th row of a matrix X with dimensions $100\\times 200$.\n",
    "\n",
    "\n",
    "* Plot two realizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pi = np.pi\n",
    "\n",
    "K = 100      # number of realizations\n",
    "T = 200      # total length of signal\n",
    "T0 = 100     # signal period in samples\n",
    "Tplot = 200  # samples to plot\n",
    "X = np.zeros((K, T))\n",
    "             \n",
    "# generate random realizations\n",
    "for i in range(K): # same as \"for i in np.arange(K):\" in this case\n",
    "    np.random.seed(i)\n",
    "    X[i, :] = np.sin(2*pi*np.arange(T)/T0 + 2*pi*np.random.random())\n",
    "\n",
    "Xabsmax = np.max(abs(X))\n",
    "\n",
    "# plot two signal realizations\n",
    "for i in range(2):\n",
    "    plt.figure(figsize = (10, 4))\n",
    "    plt.stem(X[i, range(Tplot)])\n",
    "    plt.xlabel('$t$')\n",
    "    plt.ylabel(r'$x(t, \\theta_' + str(i) + ')$')\n",
    "    plt.title(r'$X(t)$, realization $\\theta_' + str(i) + '$')\n",
    "    plt.axis([-1,Tplot, -1.1*Xabsmax, 1.1*Xabsmax])\n",
    "    plt.grid()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and variance\n",
    "\n",
    "From a finite number of realizations $\\{x(t, \\theta_k)\\}_{k=1}^K$, the mean and variance can be approximated by the **sample averages** as follows:\n",
    "$$\\mu_X(t) = \\int_{-\\infty}^{+\\infty}xf_X(x;t)dx = \\int_{-\\infty}^{+\\infty} x(t, \\theta)f_\\Theta(\\theta)d\\theta \\approx \\frac{1}{K} \\sum_{k=1}^K x(t, \\theta_k)$$\n",
    "\n",
    "$$\\sigma^2_X(t) = \\int_{-\\infty}^{+\\infty}[x - \\mu_X(t)]^2 f_X(x;t)dx = \\int_{-\\infty}^{+\\infty} [x(t, \\theta) - \\mu_X(t)]^2f_\\Theta(\\theta)d\\theta \\approx \\frac{1}{K} \\sum_{k=1}^K [x(t, \\theta_k) - \\mu_X(t)]^2 = \\frac{1}{K} \\sum_{k=1}^K x(t, \\theta_k)^2 - \\mu^2_X(t)$$\n",
    "\n",
    "where $\\{\\theta_k\\}_{k=1}^K$ are the realizations of random variable $\\theta$ characterizing the random process.\n",
    "\n",
    "* Compute and plot the mean $\\mu_X(t)$ and variance $\\sigma^2_X(t)$ of random process $X(t)$.\n",
    "\n",
    "\n",
    "* What can we say about the variability of these statistics?\n",
    "\n",
    "*Remark 1:* results may show some variance due to estimation from a finite sample size.\n",
    "\n",
    "*Remark 2:* the sample averages used here are not to be confused with the time averages used when studying and exploiting ergodicity (see exercise below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tval = np.arange(T)\n",
    "mu = np.zeros(T)\n",
    "sg2 = np.zeros(T)\n",
    "\n",
    "mu = X.mean(axis = 0) # compute average over realizations (axis = 0)\n",
    "X2 = X**2\n",
    "sg2 = X2.mean(axis = 0) - mu**2\n",
    "\n",
    "# plot mean\n",
    "plt.figure(figsize = (10, 4))\n",
    "plt.stem(Tval, mu)\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$\\mu_X(t)$')\n",
    "plt.title('Mean, $\\mu_X(t)$')\n",
    "plt.axis([0, T, -1.1*Xabsmax, 1.1*Xabsmax]);\n",
    "plt.grid()\n",
    "    \n",
    "# plot variance\n",
    "plt.figure(figsize = (10, 4))\n",
    "plt.stem(Tval, sg2)\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$\\sigma_X^2(t)$')\n",
    "plt.title('Variance, $\\sigma_X^2(t)$')\n",
    "plt.axis([0, T, -1.1*Xabsmax, 1.1*Xabsmax]);\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation sequence (ACS)\n",
    "\n",
    "The ACS is defined as $$R_X(t_1, t_2) = \\mathrm{E}\\{X(t_1)X(t_2)\\}.$$\n",
    "\n",
    "When analyzing real data, the expectation can be approximated by the sample average:\n",
    "$$\\mathrm{E}\\{X(t_1)X(t_2)\\} = \\iint_{-\\infty}^{+\\infty} x_1x_2f_X(x_1, x_2; t_1, t_2)dx_1dx_2 = \\int_{-\\infty}^{+\\infty} x(t_1, \\theta)x(t_2, \\theta)f_\\Theta(\\theta)d\\theta \\approx \\frac{1}{K} \\sum_{k=1}^K x(t_1, \\theta_k)x(t_2, \\theta_k).$$\n",
    "\n",
    "* Compute and plot the autocorrelation sequence for $t_1 = 0$ and $t_2 \\in [0, 100[$. \n",
    "\n",
    "\n",
    "* Repeat for $t_1 = 10$ and then for $t_1 = 20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = 21   # maximum number of lags in t1\n",
    "T2 = 100  # number of lags in t2\n",
    "\n",
    "T1val = np.array([0, 10, 20])\n",
    "T2val = np.arange(T2)\n",
    "Rx = np.zeros((T1, T2))\n",
    "\n",
    "for t1 in T1val:\n",
    "    for t2 in T2val: # same as \"for t in range(T2):\" in this case\n",
    "        c = 0\n",
    "        for k in range(K):\n",
    "            c += X[k, t1]*X[k, t2]\n",
    "        Rx[t1, t2] = c/K\n",
    "        \n",
    "Rxabsmax = abs(Rx).max()\n",
    "\n",
    "# plot ACS\n",
    "for t1 in T1val:\n",
    "    plt.figure(figsize = (10, 4))\n",
    "    plt.stem(T2val, Rx[t1, :])\n",
    "    plt.xlabel('$t_2$')\n",
    "    plt.ylabel('$R_X(' + str(t1) + ', t_2)$')\n",
    "    plt.title('Autocorrelation sequence, $t_1 = $' + str(t1))\n",
    "    plt.axis([0, T2, -1.1*Rxabsmax, 1.1*Rxabsmax]);\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide sense stationarity (WSS)\n",
    "A random process $X(t)$ is said to be WSS if $\\mu_X(t)$ is constant and $R_X(t_1, t_2)$ only depends on the time lag $\\tau \\overset{\\mathrm{def}}{=} (t_2 - t_1)$.\n",
    "\n",
    "* Show $R_X(t_1, t_2)$ for different values of $(t_1, t_2)$ with the same lag $\\tau = (t_2 - t_1)$. Justify that process $X(t)$ is WSS.\n",
    "\n",
    "*Remark:* results for different values of $(t_1, t_2)$ with the same time lag may show some variance due to the finite sample size used in ACS estimation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('>>> Lag tau = 0')\n",
    "print('R_X(0, 0) = ', round(Rx[0, 0], 4))\n",
    "print('R_X(10, 10) = ', round(Rx[10, 10],4))\n",
    "print('R_X(20, 20) = ', round(Rx[20, 20],4))\n",
    "\n",
    "print('>>> Lag tau = 10')\n",
    "print('R_X(0, 10) = ', round(Rx[0, 10],4))\n",
    "print('R_X(10, 20) = ', round(Rx[10, 20], 4))\n",
    "print('R_X(20, 30) = ', round(Rx[20, 30], 4))\n",
    "\n",
    "print('>>> Lag tau = 20')\n",
    "print('R_X(0, 20) = ', round(Rx[0, 20], 4))\n",
    "print('R_X(10, 30) = ', round(Rx[10, 30], 4))\n",
    "print('R_X(20, 40) = ', round(Rx[20, 40], 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute and plot the autocorrelation sequence $R_X(\\tau) = R_X(0, \\tau)$ for $\\tau\\in [-100, 100]$.\n",
    "\n",
    "*Hint:* the ACS presents Hermitian symmetry, i.e., $R_X(-\\tau) = R_X^*(\\tau)$, where $(\\cdot)^*$ denotes complex conjugation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rxtau = Rx[0, :]\n",
    "Rxtau = np.concatenate((Rxtau[range(T2-1, 0, -1)], Rxtau)) # exploit even symmetry to get negative lag values\n",
    "tau = np.arange(-T2+1, T2)\n",
    "\n",
    "plt.figure(figsize = (10, 4))\n",
    "plt.stem(tau, Rxtau)\n",
    "plt.xlabel(r'$\\tau$')\n",
    "plt.ylabel(r'$R_X(\\tau)$')\n",
    "plt.title(r'Autocorrelation sequence $R_X(\\tau)$')\n",
    "plt.axis([-T2, T2, -1.1*max(abs(Rxtau)), 1.1*max(abs(Rxtau))]);\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergodicity\n",
    "\n",
    "**Time averages** are defined as\n",
    "\n",
    "$$m_X \\overset{\\mathrm{def}}{=}{}<x(t)>{}= \\frac{1}{T}\\sum_{t=0}^{T-1} x(t)$$\n",
    "\n",
    "$$\\Gamma_X(\\tau) \\overset{\\mathrm{def}}{=} {} <x(t)x(t+\\tau)> {} = \\frac{1}{T}\\sum_{t=0}^{T-1} x(t)x(t+\\tau).$$\n",
    "\n",
    "\n",
    "* For any realization of the random process, compute the above time averages. Compute $\\Gamma_X(\\tau)$ in the interval $\\tau\\in]-100, 100[$.\n",
    "\n",
    "\n",
    "* Compare time averages $m_X$ and $\\Gamma_X(\\tau)$ with ensemble averages $\\mu_X$ and $R_X(\\tau)$. Is the process ergodic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.mean(axis = 1)          # compute average along time (axis = 1)\n",
    "Gamma = np.zeros((K, 2*T2-1))\n",
    "\n",
    "for k in range(K):\n",
    "    for i in range(-T2+1, T2):\n",
    "        for t in range(max([0, -i]), min([T, T-i])):\n",
    "            Gamma[k, T2-1+i] += X[k, t]*X[k, t+i]\n",
    "\n",
    "Gamma /= T\n",
    "\n",
    "k = 0   # chosen realization for visualization\n",
    "print('m_X = ', round(m[k], 4))\n",
    "print('mu_X = ', round(mu.mean(), 4))\n",
    "\n",
    "plt.figure(figsize = (10, 4))\n",
    "plt.stem(tau, Rxtau)\n",
    "plt.stem(tau, Gamma[k,:], 'r')\n",
    "plt.xlabel(r'$\\tau$')\n",
    "plt.ylabel(r'$R_X(\\tau), \\Gamma_X(\\tau)$')\n",
    "plt.title('Ergodicity - ACS estimates from sample and time averages')\n",
    "plt.legend(('$R_X$', '$\\Gamma_X$'))\n",
    "plt.axis([-T2, T2, -1.1*max(abs(Rxtau)), 1.1*max(abs(Rxtau))]);\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation accuracy: bias, variance and mean square error\n",
    "\n",
    "Time averages $m_X$ and $\\Gamma_X(\\tau)$ can be considered as estimates of the ensemble averages $\\mu_X$ and $R_X(\\tau)$, respectively.\n",
    "\n",
    "Each realization $x(t, \\theta_k)$ of the random process $X(t)$ generates a different value of $m_X$ and $\\Gamma_X(\\tau)$. Hence, these quantities are random variables.\n",
    "\n",
    "* Compute the bias, variance and mean square error (MSE) of $m_X$ and $\\Gamma_X(\\tau)$ as estimators of $\\mu_X$ and $R_X(\\tau)$, respectively. Plot the results for $\\Gamma_X(\\tau)$ as a function of $\\tau$.\n",
    "\n",
    "\n",
    "* Why does the estimation error degrade as $|\\tau|$ increases?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbias = m.mean() - mu.mean() # compute mean along realizations\n",
    "mvar = m.var()               # compute variance along realizations\n",
    "mmse = mbias**2 + mvar\n",
    "\n",
    "print('bias{m_X} =', round(mbias, 4))\n",
    "print('var{m_X} =', round(mvar, 4))\n",
    "print('MSE{m_X} = ', round(mmse, 4))\n",
    "\n",
    "Gammabias = Gamma.mean(axis = 0) - Rxtau\n",
    "Gammavar = Gamma.var(axis = 0)\n",
    "Gammamse = Gammabias**2 + Gammavar\n",
    "\n",
    "plt.figure(figsize = (10, 4))\n",
    "plt.stem(tau, Gammabias)\n",
    "plt.stem(tau, Gammavar, 'r')\n",
    "plt.stem(tau, Gammamse, 'g')\n",
    "plt.xlabel(r'$\\tau$')\n",
    "plt.ylabel(r'$\\Gamma_X(\\tau)$ - bias, variance and MSE')\n",
    "plt.title('Ergodicity - ACS estimate from time average')\n",
    "plt.legend(('bias', 'variance', 'MSE'))\n",
    "plt.axis([-T2, T2, -1.1*max(abs(Rxtau)), 1.1*max(abs(Rxtau))]);\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further work\n",
    "\n",
    "* Repeat the above experiments with a Gaussian random process $X(t)$, where each realization $x(t, \\theta)$ is composed of a sequence of independent and identically distributed normalized Gaussian random variables.\n",
    "\n",
    "\n",
    "* Justify the shape of the ACS and anticipate the shape of the PSD. Why is this process called *'white'*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random realizations\n",
    "for i in range(100):\n",
    "    np.random.seed(i)\n",
    "    X[i, :] = np.random.normal(size = T)\n",
    "\n",
    "Xabsmax = np.max(abs(X))\n",
    "\n",
    "# plot two signal realizations\n",
    "for i in range(2):\n",
    "    plt.figure(figsize = (10, 4))\n",
    "    plt.stem(X[i, range(Tplot)])\n",
    "    plt.xlabel('$t$')\n",
    "    plt.ylabel(r'$x(t, \\theta_' + str(i) + ')$')\n",
    "    plt.title(r'$X(t)$, realization $\\theta_' + str(i) + '$')\n",
    "    plt.axis([-1,Tplot, -1.1*Xabsmax, 1.1*Xabsmax])\n",
    "    plt.grid()\n",
    "\n",
    "\n",
    "# etc\n",
    "# .\n",
    "# .\n",
    "# .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

